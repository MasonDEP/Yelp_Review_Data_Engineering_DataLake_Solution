# Yelp Review Data Engineering DataLake Solution

Tech Stack: Python, SQL, Spark, AWS S3, Databricks

## Overview

This project is a data engineering solution for a popular music streaming app to store, analyze and gather insight from its user activity data.The project aims to understand what songs users are listening to. The challenge of the company is the lack of an easy way to query their data that are stored purely in JSON format.


## Source Data

The source Data for this project are two sets of JSON files that contains information realted to the songs and user play history respectively.

#### Song JSON files
Song files are partitioned by the first three letters of each song's track ID and are a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).  The following filepath and its content are given as an example:

> song_data/A/A/B/TRAABJL12903CDCF1A.json

> {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

#### Log JSON files

Log files are partitioned by year and month as follows and is generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.:

> log_data/2018/11/2018-11-13-events.json


## Table Schema

#### Fact Table

| songplays |
| --- |
| songplay_id |
| start_time |
| user_id |
| level |
| song_id |
| artist_id |
| session_id |
| location |
| user_agent |
> Records in log data associated with song plays i.e. records with page

#### Dimension Tables

| users  |
| --- |
| user_id |
| first_name |
| last_name |
| gender |
| level |
> App users 

| songs   |
| --- |
| song_id |
| title |
| artist_id |
| year |
| duration |
> Songs in music database

| artists    |
| --- |
| artist_id |
| name |
| location |
| lattitude |
| longitude |
> Artists in music database

| time     |
| --- |
| start_time |
| hour |
| day |
| week |
| month |
| year |
| weekday |

## File descriptions

`data` folder contains partitioned song and log JSON files.

`sql_queries.py` contains SQL queries that are execued by other files.

`create_tables.py` drops and reinitialises the database environment using queries from `sql_queries.py`.

`etl.py` performs the massive data extraction from JSON files in `data` folder, transforms it into the defined types and tables groups then uploads it to the postgres Database.

`Data_Engineering_Cassandra.py` model the data from `event_datafile_new.csv` by 3 different frequently used queries and store the data into a Apache Cassandra Keyspace.

`event_datafile_new.csv` is an extracted log sample file in csv format.

## Running the ETL pipeline for Postgres DB

1  Ensure the `data` folder and all project files are downloaded and that all dependencies are met. Replace the given connection strings in `create_tables.py` and `etl.py` with your own, pointing to a postgres database server you have set up.Replace the JSON data file location in `etl.py` with your own loacl directory.

2  Run `create_tables.py` to reinitialise the database.

3  Run `etl.py` to start the ETL data pipeline from JSON logs in the `data` folder to the postgres database
